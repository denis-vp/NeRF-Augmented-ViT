{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RoMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 70446,
     "status": "ok",
     "timestamp": 1749133697346,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "vqlVNU1Mre9l",
    "outputId": "d06c76b4-ac06-43af-f4c2-b6df91268af5"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Parskatt/RoMa.git\n",
    "%cd RoMa\n",
    "!pip install -q -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 45521,
     "status": "ok",
     "timestamp": 1749133742865,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "CAGAANqzsSO8"
   },
   "outputs": [],
   "source": [
    "SCENE = \"0080\"\n",
    "SCENE_DIR = f\"/content/{SCENE}\"\n",
    "\n",
    "!unzip -q /content/drive/MyDrive/thesis/Datasets/{SCENE}-nerf.zip -d {SCENE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1210,
     "status": "ok",
     "timestamp": 1749133744080,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "S0Q7OYuA0Sjl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1749133744201,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "wiD0JcgpuSfl",
    "outputId": "3d27b66f-1984-42d7-b077-cd7a69f667df"
   },
   "outputs": [],
   "source": [
    "scene_dir = f\"/content/{SCENE}\"\n",
    "images_dir = os.path.join(scene_dir, 'images')\n",
    "depths_dir = os.path.join(scene_dir, 'depths')\n",
    "camera_json = os.path.join(scene_dir, 'cameras.json')\n",
    "\n",
    "with open(camera_json, 'r') as f:\n",
    "    cam = json.load(f)\n",
    "\n",
    "# Extract camera parameters\n",
    "fl_x, fl_y = cam['fl_x'], cam['fl_y']\n",
    "cx, cy = cam['cx'], cam['cy']\n",
    "K = [\n",
    "    [fl_x, 0.0, cx],\n",
    "    [0.0, fl_y, cy],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]\n",
    "\n",
    "# Collect all images, depth maps, and poses\n",
    "imgs = sorted([\n",
    "    fn for fn in os.listdir(images_dir)\n",
    "    if fn.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "keys = [fn.replace('.color.png', '') for fn in imgs if fn.endswith('.color.png')]\n",
    "\n",
    "depth_map = {}\n",
    "for fn in os.listdir(depths_dir):\n",
    "    if fn.lower().endswith('.npy'):\n",
    "        base = fn.rsplit('.depth.npy', 1)[0]\n",
    "        depth_map[base] = fn\n",
    "\n",
    "poses = {}\n",
    "for frame in cam.get('frames', []):\n",
    "    key = os.path.basename(frame['file_path']).replace('.jpg', '').replace('.color.png', '')\n",
    "    poses[key] = np.array(frame['transform_matrix'])\n",
    "\n",
    "# Select the train and test frames\n",
    "train_keys, test_keys = train_test_split(keys, test_size=0.1, random_state=42)\n",
    "\n",
    "def generate_balanced_pairs(keys, max_pairs_per_image=25):\n",
    "    key_list = sorted(list(keys))\n",
    "    used_counts = defaultdict(int)\n",
    "    roma_pairs = []\n",
    "\n",
    "    for i, key1 in enumerate(key_list):\n",
    "        available = [k for k in key_list if k != key1]\n",
    "        sampled_keys = random.sample(available, min(max_pairs_per_image, len(available)))\n",
    "\n",
    "        for key2 in sampled_keys:\n",
    "            # Ensure symmetric limit\n",
    "            if used_counts[key1] >= max_pairs_per_image or used_counts[key2] >= max_pairs_per_image:\n",
    "                continue\n",
    "\n",
    "            img1 = f\"{key1}.color.png\"\n",
    "            img2 = f\"{key2}.color.png\"\n",
    "            d1 = depth_map.get(key1)\n",
    "            d2 = depth_map.get(key2)\n",
    "            T1 = poses.get(key1)\n",
    "            T2 = poses.get(key2)\n",
    "\n",
    "            if any(x is None for x in (d1, d2, T1, T2)):\n",
    "                continue\n",
    "\n",
    "            rel = (np.linalg.inv(T2) @ T1).tolist()\n",
    "\n",
    "            roma_pairs.append({\n",
    "                'img1': os.path.join('images', img1),\n",
    "                'img2': os.path.join('images', img2),\n",
    "                'im_A_depth': os.path.join('depths', d1),\n",
    "                'im_B_depth': os.path.join('depths', d2),\n",
    "                'K_A': K,\n",
    "                'K_B': K,\n",
    "                'rel_pose': rel\n",
    "            })\n",
    "\n",
    "            used_counts[key1] += 1\n",
    "            used_counts[key2] += 1\n",
    "\n",
    "    return roma_pairs\n",
    "\n",
    "# Generate balanced pairs for training and validation sets\n",
    "random.seed(42)\n",
    "train_pairs = generate_balanced_pairs(train_keys, max_pairs_per_image=25)\n",
    "val_pairs = generate_balanced_pairs(test_keys, max_pairs_per_image=25)\n",
    "\n",
    "out_train = os.path.join(scene_dir, 'roma_pairs_train.json')\n",
    "out_val = os.path.join(scene_dir, 'roma_pairs_val.json')\n",
    "\n",
    "with open(out_train, 'w') as f:\n",
    "    json.dump(train_pairs, f, indent=2)\n",
    "with open(out_val, 'w') as f:\n",
    "    json.dump(val_pairs, f, indent=2)\n",
    "\n",
    "print(f\"Exported {len(train_pairs)} training pairs to {out_train}\")\n",
    "print(f\"Exported {len(val_pairs)} validation pairs to {out_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the NeRFRoMaDataset class file in the `romatch` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1749133744265,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "wm_qKtOusudT"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > romatch/datasets/nerf_roma_dataset.py << 'EOF'\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NeRFRoMaDataset(Dataset):\n",
    "    def __init__(self, cfg, split=\"train\", transform=None, depth_transform=None):\n",
    "        self.root_dir = cfg['root_dir']\n",
    "        self.transform = transform\n",
    "        self.depth_transform = depth_transform\n",
    "\n",
    "        json_path = os.path.join(self.root_dir, f\"roma_pairs_{split}.json\")\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(f\"Missing file: {json_path}\")\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.pairs = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "        im_A = Image.open(os.path.join(self.root_dir, pair[\"img1\"])).convert(\"RGB\")\n",
    "        im_B = Image.open(os.path.join(self.root_dir, pair[\"img2\"])).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            im_A, im_B = self.transform((im_A, im_B))\n",
    "        else:\n",
    "            im_A = torch.from_numpy(np.array(im_A)).permute(2, 0, 1).float().div(255.0)\n",
    "            im_B = torch.from_numpy(np.array(im_B)).permute(2, 0, 1).float().div(255.0)\n",
    "\n",
    "        depth_A_np = np.load(os.path.join(self.root_dir, pair[\"im_A_depth\"])).astype(np.float32)\n",
    "        depth_B_np = np.load(os.path.join(self.root_dir, pair[\"im_B_depth\"])).astype(np.float32)\n",
    "\n",
    "        if self.depth_transform:\n",
    "            depth_A, depth_B = self.depth_transform((depth_A_np, depth_B_np))\n",
    "        else:\n",
    "            depth_A = torch.from_numpy(depth_A_np)\n",
    "            depth_B = torch.from_numpy(depth_B_np)\n",
    "\n",
    "        K1 = torch.tensor(pair[\"K_A\"], dtype=torch.float32)\n",
    "        K2 = torch.tensor(pair[\"K_B\"], dtype=torch.float32)\n",
    "        T_1to2 = torch.tensor(pair[\"rel_pose\"], dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            'im_A': im_A.clone(),\n",
    "            'im_B': im_B.clone(),\n",
    "            'im_A_depth': depth_A.clone(),\n",
    "            'im_B_depth': depth_B.clone(),\n",
    "            'K1': K1.clone(),\n",
    "            'K2': K2.clone(),\n",
    "            'T_1to2': T_1to2.clone(),\n",
    "        }\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8814,
     "status": "ok",
     "timestamp": 1749133753081,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "P62kSJtc0n_x"
   },
   "outputs": [],
   "source": [
    "# ------- IF RELOAD IS NEEDED -------\n",
    "# In case Colab uses cached version\n",
    "\n",
    "import importlib\n",
    "import romatch.datasets.nerf_roma_dataset\n",
    "importlib.reload(romatch.datasets.nerf_roma_dataset)\n",
    "\n",
    "from romatch.datasets.nerf_roma_dataset import NeRFRoMaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3012,
     "status": "ok",
     "timestamp": 1749133756098,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "57e1rabX0wZ7"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import sys\n",
    "import gc\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from romatch.utils import tensor_to_pil\n",
    "\n",
    "import romatch\n",
    "from romatch.datasets.nerf_roma_dataset import NeRFRoMaDataset\n",
    "from romatch.losses.robust_loss import RobustLosses\n",
    "from romatch import tiny_roma_v1_outdoor, roma_outdoor\n",
    "from romatch.utils import get_tuple_transform_ops, get_depth_tuple_transform_ops\n",
    "from romatch.utils.utils import to_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5907,
     "status": "ok",
     "timestamp": 1749133762009,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "sYxPn6pv0yUr"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# GLOBALS\n",
    "# -----------------------------------------------------------------------------\n",
    "save_dir = f\"/content/drive/MyDrive/thesis/RoMa/Checkpoints_Metrics/{SCENE}_tiny\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "loss_log_path = os.path.join(save_dir, \"losses.json\")\n",
    "loss_log = {}\n",
    "if os.path.exists(loss_log_path):\n",
    "    with open(loss_log_path, \"r\") as f:\n",
    "        loss_log = json.load(f)\n",
    "\n",
    "metrics_log_path = os.path.join(save_dir, \"metrics_log.json\")\n",
    "metrics_log = {}\n",
    "if os.path.exists(metrics_log_path):\n",
    "    with open(metrics_log_path, \"r\") as f:\n",
    "        metrics_log = json.load(f)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TRAIN STEP\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_step(train_batch, model, objective, optimizer, grad_scaler, grad_clip_norm=1., **kwargs):\n",
    "    optimizer.zero_grad()\n",
    "    with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "        out = model(train_batch)\n",
    "        l = objective(out, train_batch)\n",
    "        l = torch.clamp(l, min=0.0, max=10.0)\n",
    "    grad_scaler.scale(l).backward()\n",
    "    grad_scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "    grad_scaler.step(optimizer)\n",
    "    grad_scaler.update()\n",
    "    return {\"train_out\": out, \"train_loss\": l.item()}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EVAL\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_pck(pred_kpts, gt_kpts, thresholds=[1, 3, 5]):\n",
    "    dists = np.linalg.norm(pred_kpts - gt_kpts, axis=2).ravel()\n",
    "    pck = {}\n",
    "    for t in thresholds:\n",
    "        pck[f'PCK@{t}px'] = np.mean(dists < t)\n",
    "    return pck\n",
    "\n",
    "def compute_auc(errors, max_threshold=10):\n",
    "    errs = np.sort(errors)\n",
    "    recall = np.linspace(0, 1, len(errs))\n",
    "    ths    = np.linspace(0, max_threshold, len(errs))\n",
    "    mask = errs <= max_threshold\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    return np.trapz(recall[mask], ths[mask]) / max_threshold\n",
    "\n",
    "def compute_maa(errors, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(1, 10, 10)\n",
    "    maa = np.mean([np.mean(errors < t) for t in thresholds])\n",
    "    return maa\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(dataloader, model, objective, epoch, config):\n",
    "    model.eval()\n",
    "    model.exact_softmax = True\n",
    "\n",
    "    all_losses, all_pcks, all_aucs, all_maas = [], [], [], []\n",
    "\n",
    "    print(f\"\\nRunning validation at epoch {epoch}\")\n",
    "    pbar = tqdm(dataloader, desc=\"Validation\", mininterval=10.0)\n",
    "    for batch in pbar:\n",
    "        batch_cuda = {k: v.cuda(non_blocking=True) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            out_train = model(batch_cuda)\n",
    "            loss = torch.clamp(objective(out_train, batch_cuda), min=0.0, max=10.0)\n",
    "            all_losses.append(loss.item())\n",
    "\n",
    "        imA_batch = batch['im_A']\n",
    "        B = imA_batch.shape[0]\n",
    "        pred_batch = []\n",
    "        for i in range(B):\n",
    "            pilA = tensor_to_pil(batch['im_A'][i], unnormalize=True)\n",
    "            pilB = tensor_to_pil(batch['im_B'][i], unnormalize=True)\n",
    "            w, _ = model.match(pilA, pilB)\n",
    "            if isinstance(w, torch.Tensor): w = w.cpu().numpy()\n",
    "            u_norm = w[..., 2].ravel()\n",
    "            v_norm = w[..., 3].ravel()\n",
    "            x_pred = (u_norm + 1.0) * (config['dataset']['coarse_res'][1] / 2.0)\n",
    "            y_pred = (v_norm + 1.0) * (config['dataset']['coarse_res'][0] / 2.0)\n",
    "            pred_kpts = np.stack([x_pred, y_pred], axis=1)\n",
    "            pred_batch.append(pred_kpts)\n",
    "\n",
    "        pred_all = np.stack(pred_batch, axis=0)\n",
    "        gt_all = batch['gt_kpts'].cpu().numpy().reshape(B, -1, 2)\n",
    "\n",
    "        H, W = config['dataset']['coarse_res']\n",
    "        valid_mask = (\n",
    "            (gt_all[..., 0] >= 0) & (gt_all[..., 0] < W) &\n",
    "            (gt_all[..., 1] >= 0) & (gt_all[..., 1] < H)\n",
    "        )\n",
    "\n",
    "        dists = np.linalg.norm(pred_all - gt_all, axis=2)\n",
    "        dists = dists[valid_mask]\n",
    "\n",
    "        if dists.size > 0:\n",
    "            pbar.set_postfix(min=dists.min(), mean=dists.mean(), max=dists.max())\n",
    "            all_aucs.append(compute_auc(dists))\n",
    "            all_maas.append(compute_maa(dists))\n",
    "        else:\n",
    "            pbar.set_postfix_str(\"No valid correspondences\")\n",
    "            continue\n",
    "\n",
    "        all_pcks.append(compute_pck(pred_all, gt_all))\n",
    "\n",
    "    if len(all_aucs) == 0 or len(all_maas) == 0 or len(all_pcks) == 0:\n",
    "        print(\"No valid samples found during evaluation.\")\n",
    "        return float('inf')\n",
    "\n",
    "    avg_loss = np.mean(all_losses)\n",
    "    pck_keys = list(all_pcks[0].keys())\n",
    "    avg_pck = {k: np.mean([m[k] for m in all_pcks]) for k in pck_keys}\n",
    "    avg_auc = np.mean(all_aucs); avg_maa = np.mean(all_maas)\n",
    "\n",
    "    metrics_log[f\"epoch_{epoch}\"] = {\"avg_loss\": avg_loss, **avg_pck, \"AUC\": avg_auc, \"mAA\": avg_maa}\n",
    "    with open(metrics_log_path, \"w\") as f: json.dump(metrics_log, f, indent=2)\n",
    "\n",
    "    pck_str = \" \".join(f\"{k}={v:.3f}\" for k, v in avg_pck.items())\n",
    "    print(f\"Eval epoch {epoch}: loss={avg_loss:.4f} AUC={avg_auc:.4f} mAA={avg_maa:.4f} {pck_str}\")\n",
    "\n",
    "    model.exact_softmax = False\n",
    "    return avg_loss\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TRAIN EPOCH\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_epoch(dataloader, model, objective, optimizer, lr_scheduler, scaler, epoch, config):\n",
    "    model.train(True)\n",
    "    print(f\"\\nStarting epoch {epoch}/{config['train']['epochs']}\")\n",
    "\n",
    "    epoch_losses = []\n",
    "    pbar = tqdm(\n",
    "        dataloader,\n",
    "        desc=f\"Epoch {epoch}/{config['train']['epochs']}\",\n",
    "        mininterval=10.0,\n",
    "        leave=False,\n",
    "        file=sys.stdout\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = to_cuda(batch)\n",
    "        res = train_step(batch, model, objective, optimizer, scaler)\n",
    "\n",
    "        if not torch.isfinite(torch.tensor(res['train_loss'])):\n",
    "            raise Exception(\"NaN/Inf loss\")\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{res['train_loss']:.4f}\")\n",
    "        epoch_losses.append(res['train_loss'])\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        romatch.GLOBAL_STEP += romatch.STEP_SIZE\n",
    "\n",
    "    ckpt_path = os.path.join(save_dir, f\"{config['model']['name']}_ep{epoch:03d}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'grad_scaler_state_dict': scaler.state_dict(),\n",
    "        'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    }, ckpt_path)\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    print(f\"Finished epoch {epoch}: avg batch loss = {avg_loss:.4f}\")\n",
    "\n",
    "    loss_log[f\"epoch_{epoch}\"] = {\n",
    "        \"batch_losses\": epoch_losses,\n",
    "        \"avg_loss\": avg_loss\n",
    "    }\n",
    "    with open(loss_log_path, \"w\") as f:\n",
    "        json.dump(loss_log, f, indent=2)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TRAIN K EPOCHS\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_k_epochs(start_epoch, end_epoch, train_loader, val_loader, model, objective, optimizer, lr_scheduler, scaler, config):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = config[\"train\"].get(\"early_stopping_patience\", None)\n",
    "    patience_counter = 0\n",
    "\n",
    "    eval_every = config[\"train\"].get(\"eval_every\", 3)\n",
    "\n",
    "    for ep in range(start_epoch, end_epoch + 1):\n",
    "        train_epoch(train_loader, model, objective, optimizer, lr_scheduler, scaler, ep, config)\n",
    "\n",
    "        should_eval = (ep % eval_every == 0) or (ep == end_epoch) or (ep == 1)\n",
    "\n",
    "        should_eval = True\n",
    "\n",
    "        if should_eval:\n",
    "            val_loss = eval_epoch(val_loader, model, objective, ep, config)\n",
    "\n",
    "            if patience is not None:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"Early stopping patience {patience_counter}/{patience}\")\n",
    "                    if patience_counter >= patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1749133762059,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "YoylHSpVg2Cu"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\": {\n",
    "        \"name\": \"nerf_roma\",\n",
    "        \"root_dir\": \"/content/0080\",\n",
    "        \"train_batch_size\": 40,\n",
    "        \"val_batch_size\": 40,\n",
    "        \"coarse_res\": (560, 560),\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"tiny_roma\",\n",
    "        \"freeze_backbone\": False,\n",
    "        \"use_pretrained\": True\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 2e-6,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"early_stopping_patience\": 10,\n",
    "        \"lr_scheduler\": {\n",
    "            \"name\": \"cosine\",\n",
    "            \"T_max\": 20,\n",
    "            \"eta_min\": 1e-6\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681169,
     "status": "ok",
     "timestamp": 1749149851775,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "od3WDvjl0zrD",
    "outputId": "42353fee-1bbc-4e7e-dee2-182bb5eca7a1"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# DEVICE, TRANSFORMS, DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.ipc_collect()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COARSE_RES = config[\"dataset\"][\"coarse_res\"]\n",
    "im_transform  = get_tuple_transform_ops(resize=COARSE_RES, normalize=True)\n",
    "depth_trans   = get_depth_tuple_transform_ops(resize=COARSE_RES, normalize=False)\n",
    "def depth_transform_pair(im_tuple):\n",
    "    tA = torch.from_numpy(im_tuple[0]).float()[None,None]\n",
    "    tB = torch.from_numpy(im_tuple[1]).float()[None,None]\n",
    "    oA, oB = depth_trans((tA, tB))\n",
    "    return oA.squeeze(), oB.squeeze()\n",
    "\n",
    "train_dataset = NeRFRoMaDataset(\n",
    "    config[\"dataset\"],\n",
    "    split=\"train\",\n",
    "    transform=im_transform,\n",
    "    depth_transform=depth_transform_pair,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"dataset\"][\"train_batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dataset = NeRFRoMaDataset(\n",
    "    config[\"dataset\"],\n",
    "    split=\"val\",\n",
    "    transform=im_transform,\n",
    "    depth_transform=depth_transform_pair,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"dataset\"][\"val_batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODEL, OPTIMIZER, SCHEDULER, SCALER\n",
    "# -----------------------------------------------------------------------------\n",
    "is_model_tiny = True if config[\"model\"][\"name\"] == \"tiny_roma\" else False\n",
    "model = (tiny_roma_v1_outdoor if is_model_tiny else roma_outdoor)(device=device).to(device)\n",
    "\n",
    "criterion = RobustLosses(); criterion.local_largest_scale = -1\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config[\"train\"][\"lr\"],\n",
    "    weight_decay=config[\"train\"].get(\"weight_decay\", 0)\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max = config[\"train\"][\"epochs\"] * len(train_loader),\n",
    "    eta_min = config[\"train\"][\"lr_scheduler\"].get(\"eta_min\", 0)\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOAD CHECKPOINT IF AVAILABLE\n",
    "# -----------------------------------------------------------------------------\n",
    "checkpoint_files = sorted(glob.glob(f\"{save_dir}/{config['model']['name']}_ep*.pth\"))\n",
    "start_epoch = 1\n",
    "\n",
    "if checkpoint_files:\n",
    "    latest_ckpt = checkpoint_files[-1]\n",
    "    print(f\"üîÅ Resuming from checkpoint: {latest_ckpt}\")\n",
    "    checkpoint = torch.load(latest_ckpt)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    if 'grad_scaler_state_dict' in checkpoint:\n",
    "        scaler.load_state_dict(checkpoint['grad_scaler_state_dict'])\n",
    "\n",
    "    OVERRIDE_LR = False\n",
    "\n",
    "    if OVERRIDE_LR:\n",
    "        new_lr = config[\"train\"][\"lr\"]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "        print(f\"Overriding LR to {new_lr:.2e} after loading checkpoint\")\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max = config[\"train\"][\"epochs\"] * len(train_loader),\n",
    "            eta_min = config[\"train\"][\"lr_scheduler\"].get(\"eta_min\", 0)\n",
    "        )\n",
    "else:\n",
    "    print(\"No checkpoint found, starting from scratch.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RUN TRAINING\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"Starting training: {config['train']['epochs']} epochs, {len(train_loader)} steps/epoch\")\n",
    "train_k_epochs(\n",
    "    start_epoch = start_epoch,\n",
    "    end_epoch   = config[\"train\"][\"epochs\"],\n",
    "    train_loader= train_loader,\n",
    "    val_loader  = val_loader,\n",
    "    model       = model,\n",
    "    objective   = criterion,\n",
    "    optimizer   = optimizer,\n",
    "    lr_scheduler= scheduler,\n",
    "    scaler      = scaler,\n",
    "    config      = config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOKi2Pj3vLkHHYEig+cdRN2",
   "gpuType": "L4",
   "machine_shape": "hm",
   "mount_file_id": "1-p2Skcis7H-3X90xA-zEtwkR6NJDSKK4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
