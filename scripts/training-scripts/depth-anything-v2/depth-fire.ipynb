{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Depth-Anything-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 92601,
     "status": "ok",
     "timestamp": 1749827675976,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "gpiIptUY11v0",
    "outputId": "b99a9741-7877-444d-ff57-5697fdc2d9ea"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/DepthAnything/Depth-Anything-V2.git\n",
    "%cd Depth-Anything-V2\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 68743,
     "status": "ok",
     "timestamp": 1749827744725,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "kPKyJeJT1_Zh"
   },
   "outputs": [],
   "source": [
    "SCENE = \"fire\"\n",
    "\n",
    "!unzip -q /content/drive/MyDrive/thesis/Datasets/{SCENE}-nerf.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1749827744729,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "gniyZIPm2ADz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import zipfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1749827744754,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "2lEfALJE2A4g"
   },
   "outputs": [],
   "source": [
    "SRC_DIR = f\"/content/{SCENE}\"\n",
    "OUT_DIR = f\"/content/data\"\n",
    "\n",
    "!rm -rf {OUT_DIR}\n",
    "\n",
    "os.makedirs(os.path.join(OUT_DIR, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUT_DIR, \"depths\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUT_DIR, \"poses\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1749827744821,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "E5nngpiw2MQ_"
   },
   "outputs": [],
   "source": [
    "def read_split(file):\n",
    "    path = os.path.join(SRC_DIR, file)\n",
    "    with open(path, 'r') as f:\n",
    "        return { line.strip() for line in f if line.strip() }\n",
    "\n",
    "train_seqs = read_split(\"TrainSplit.txt\")\n",
    "test_seqs  = read_split(\"TestSplit.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read the dataset from the various dirs it is stored in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5511,
     "status": "ok",
     "timestamp": 1749827750333,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "fLLNkIKc2OPY",
    "outputId": "8c672163-206b-4059-fefd-0782e0773402"
   },
   "outputs": [],
   "source": [
    "use_nerf = False\n",
    "\n",
    "train_frames = []\n",
    "test_frames  = []\n",
    "nerf_frames  = set()\n",
    "\n",
    "for zip_path in glob.glob(os.path.join(SRC_DIR, \"seq-*.zip\")):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        raw_seq = os.path.splitext(os.path.basename(zip_path))[0]\n",
    "\n",
    "        roots = {m.split('/',1)[0] for m in zf.namelist() if not m.endswith('/')}\n",
    "\n",
    "        # Choose whether or not to use the NeRF version\n",
    "        if use_nerf and f\"{raw_seq}-nerf\" in roots:\n",
    "            desired_root = f\"{raw_seq}-nerf\"\n",
    "        elif raw_seq in roots:\n",
    "            desired_root = raw_seq\n",
    "        else:\n",
    "            print(f\"Neither '{raw_seq}' nor '{raw_seq}-nerf' found in {zip_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Identify the nerf frames\n",
    "        if use_nerf and desired_root == f\"{raw_seq}-nerf-only\":\n",
    "            nerf_only_root = f\"{raw_seq}-nerf-only\"\n",
    "\n",
    "            # list all members under the nerf-only root\n",
    "            nerf_members = [m for m in zf.namelist()\n",
    "                            if m.startswith(nerf_only_root + \"/\") and not m.endswith('/')]\n",
    "            \n",
    "            idx = int(raw_seq.split('-')[1])\n",
    "            seq_prefix = f\"seq{idx}\"\n",
    "            \n",
    "            # Add the nerf frames to the set\n",
    "            for member in nerf_members:\n",
    "                fname = os.path.basename(member)\n",
    "                if fname.startswith(\"frame-\") and fname.endswith(\".color.png\"):\n",
    "                    new_name = f\"{seq_prefix}-{fname}\"\n",
    "                    base = new_name[:-len(\".color.png\")]\n",
    "                    nerf_frames.add(base)\n",
    "\n",
    "        members = [m for m in zf.namelist()\n",
    "                   if m.startswith(desired_root + \"/\") and not m.endswith('/')]\n",
    "        if not members:\n",
    "            print(f\"No files under '{desired_root}/' in {zip_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        idx = int(raw_seq.split('-')[1])\n",
    "        seq_name = f\"sequence{idx}\"\n",
    "        seq_prefix = f\"seq{idx}\"\n",
    "        if seq_name not in train_seqs and seq_name not in test_seqs:\n",
    "            continue\n",
    "        is_train = (seq_name in train_seqs)\n",
    "\n",
    "        for member in members:\n",
    "            fname = os.path.basename(member)\n",
    "            if fname.startswith(\"frame-\") and (fname.endswith(\".color.png\") or\n",
    "                                               fname.endswith(\".depth.png\") or\n",
    "                                               fname.endswith(\".pose.txt\")):\n",
    "\n",
    "                new_fname = f\"{seq_prefix}-{fname}\"\n",
    "                base = new_fname[:-len(\".color.png\")] if fname.endswith(\".color.png\") else new_fname[:-len(\".depth.png\") if fname.endswith(\".depth.png\") else len(\".pose.txt\")]\n",
    "\n",
    "                out_subdir = \"images\" if fname.endswith(\".color.png\") else \\\n",
    "                             \"depths\" if fname.endswith(\".depth.png\") else \\\n",
    "                             \"poses\"\n",
    "                out_path = os.path.join(OUT_DIR, out_subdir, new_fname)\n",
    "                with open(out_path, \"wb\") as fo:\n",
    "                    fo.write(zf.read(member))\n",
    "\n",
    "                if fname.endswith(\".color.png\"):\n",
    "                    (train_frames if is_train else test_frames).append(base)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"train.txt\"), \"w\") as f:\n",
    "    for name in train_frames:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"val.txt\"), \"w\") as f:\n",
    "    for name in test_frames:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "# If using NeRF, create the nerf only file\n",
    "with open(os.path.join(OUT_DIR, \"nerf.txt\"), \"w\") as f:\n",
    "    for name in sorted(nerf_frames):\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "print(f\"Flattened into `{OUT_DIR}`: \"\n",
    "      f\"{len(train_frames)} train entries, \"\n",
    "      f\"{len(test_frames)} val entries, \"\n",
    "      f\"{len(nerf_frames)} nerf entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove `seq2` to increase NeRF-generate frames percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1749827750355,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "82fZz_0Ig3yj",
    "outputId": "e7dc7f9b-db72-4249-845c-5f98bda9ff70"
   },
   "outputs": [],
   "source": [
    "train_path = \"/content/data/train.txt\"\n",
    "\n",
    "with open(train_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "filtered = [line for line in lines if not line.startswith(\"seq2\")]\n",
    "\n",
    "with open(train_path, \"w\") as f:\n",
    "    f.writelines(filtered)\n",
    "\n",
    "print(f\"Removed {len(lines) - len(filtered)} entries starting with 'seq2' from {train_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the NeRFRoMaDataset class file in the `metric_depth/dataset` dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1749829973344,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "9KKm6spB28Wl"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << 'EOF' > /content/Depth-Anything-V2/metric_depth/dataset/custom.py\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "from dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop\n",
    "import logging\n",
    "\n",
    "nerf_only_dir = \"/content/data/nerf.txt\"\n",
    "\n",
    "with open(nerf_only_dir, \"r\") as f:\n",
    "    nerf_bases = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "def is_nerf_frame(base):\n",
    "    return base in nerf_bases\n",
    "\n",
    "class CustomDepthDataset(Dataset):\n",
    "    def __init__(self, names_file, root_dir, mode='train', size=(476,476),\n",
    "                 near=0.1, far=20.0):\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "        self.root = root_dir.rstrip('/')\n",
    "        self.near = near\n",
    "        self.far = far\n",
    "\n",
    "        with open(names_file, 'r') as f:\n",
    "            raw_names = [l.strip() for l in f if l.strip()]\n",
    "        cleaned = [n.rstrip('.').strip() for n in raw_names]\n",
    "\n",
    "        self.names = []\n",
    "        img_suffix   = '.color.png'\n",
    "        depth_suffix = '.depth.png'\n",
    "\n",
    "        # Build valid sample list\n",
    "        for name in cleaned:\n",
    "            img_path   = os.path.join(self.root, 'images', name + img_suffix)\n",
    "            depth_path = os.path.join(self.root, 'depths', name + depth_suffix)\n",
    "\n",
    "            if not os.path.isfile(img_path):\n",
    "                logging.warning(f\"Image not found: {img_path}, skipping.\")\n",
    "                continue\n",
    "            if not os.path.isfile(depth_path):\n",
    "                logging.warning(f\"Depth not found: {depth_path}, skipping.\")\n",
    "                continue\n",
    "            self.names.append((name, img_path, depth_path))\n",
    "\n",
    "        net_w, net_h = size\n",
    "        self.transform = Compose([\n",
    "            Resize(\n",
    "                width=net_w, height=net_h,\n",
    "                resize_target=(mode == 'train'),\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=14,\n",
    "                resize_method='lower_bound',\n",
    "                image_interpolation_method=cv2.INTER_CUBIC\n",
    "            ),\n",
    "            NormalizeImage(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            PrepareForNet(),\n",
    "            Crop(net_w) if mode == 'train' else lambda x: x\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, img_path, depth_path = self.names[idx]\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Failed to load image '{img_path}'\")\n",
    "        img = img[..., ::-1].astype(np.float32) / 255.0\n",
    "\n",
    "        depth_raw = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "        if depth_raw is None:\n",
    "            raise FileNotFoundError(f\"Failed to load depth '{depth_path}'\")\n",
    "\n",
    "        if depth_raw.ndim == 3:\n",
    "            depth_raw = depth_raw[..., 0]\n",
    "\n",
    "        if is_nerf_frame(name):\n",
    "            # For NeRF frames, depth is 16-bit unsigned integer\n",
    "            depth = depth_raw.astype(np.uint16) / 65535.0\n",
    "            depth = depth * (self.far - self.near) + self.near\n",
    "        else:\n",
    "            # The 7 scenes dataset stores depth as millimeters\n",
    "            depth = depth_raw.astype(np.float32) / 1000.0   # mm → m\n",
    "        \n",
    "        depth = np.clip(depth, self.near, self.far)\n",
    "\n",
    "        # Apply transforms\n",
    "        sample = self.transform({'image': img, 'depth': depth})\n",
    "        sample['image']      = torch.from_numpy(sample['image'])\n",
    "        sample['depth']      = torch.from_numpy(sample['depth'])\n",
    "        sample['valid_mask'] = torch.isfinite(sample['depth'])\n",
    "        sample['depth'][~sample['valid_mask']] = 0\n",
    "\n",
    "        return sample\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script\n",
    "\n",
    "It is pasted in the `metric_depth/colab_train.py` for easier running in Colab.\n",
    "As the `Depth-Anything-V2` repository does not have the option to be installed as a package, and it's modules can not be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1749829973802,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "ANG1bvsz2lVa"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << 'EOF' > /content/Depth-Anything-V2/metric_depth/colab_train.py\n",
    "import os, sys, random, logging, glob, re, json\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop\n",
    "from dataset.custom import CustomDepthDataset\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from util.loss import SiLogLoss\n",
    "from util.metric import eval_depth\n",
    "from util.utils import init_log\n",
    "\n",
    "# Configuration\n",
    "cfg = {\n",
    "    \"img_size\": 476,\n",
    "    \"min_depth\": 0.1,\n",
    "    \"max_depth\": 10.0,\n",
    "\n",
    "    \"epochs\": 60,\n",
    "    \"bs\": 16,\n",
    "    \"bs_unfrozen\": 8,\n",
    "    \"lr\": 5e-6,  # encoder LR; decoder LR will *10\n",
    "\n",
    "    \"encoder\": \"vitb\",\n",
    "\n",
    "    \"freeze_encoder\": True,\n",
    "    \"freeze_epochs\": 10,\n",
    "\n",
    "    \"lr_warmup\": 7,\n",
    "    \"lr_scheduler\": \"cosine\",\n",
    "    \"weight_decay\": 0.02\n",
    "}\n",
    "\n",
    "# Paths\n",
    "root_dir        = \"/content/data\"\n",
    "train_list      = os.path.join(root_dir, \"train.txt\")\n",
    "val_list        = os.path.join(root_dir, \"val.txt\")\n",
    "save_path       = \"/content/runs\"\n",
    "ckpt_path       = f\"/content/drive/MyDrive/thesis/Depth_Anything_V2/Checkpoints_Metrics/fire_{cfg['encoder']}\"\n",
    "pretrained_path = f\"/content/drive/MyDrive/thesis/Depth_Anything_V2/Checkpoints_Metrics/depth_anything_v2_{cfg['encoder']}.pth\"\n",
    "metrics_json    = os.path.join(ckpt_path, f\"metrics_{cfg['encoder']}.json\")\n",
    "loss_json_path = os.path.join(ckpt_path, f\"losses_{cfg['encoder']}.json\")\n",
    "\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Setup\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Load or initialize metrics history\n",
    "if os.path.exists(metrics_json):\n",
    "    with open(metrics_json, 'r') as f:\n",
    "        metrics_history = json.load(f)\n",
    "else:\n",
    "    metrics_history = {}\n",
    "\n",
    "if os.path.exists(loss_json_path):\n",
    "    with open(loss_json_path, 'r') as f:\n",
    "        loss_history = json.load(f)\n",
    "else:\n",
    "    loss_history = {}\n",
    "\n",
    "# Resume or load pretrained\n",
    "pattern = os.path.join(ckpt_path, f\"checkpoint_{cfg['encoder']}_epoch_*.pth\")\n",
    "ckpt_files = glob.glob(pattern)\n",
    "if ckpt_files:\n",
    "    def _epoch_from_path(p):\n",
    "        m = re.search(r\"epoch_(\\d+)\\.pth$\", os.path.basename(p))\n",
    "        return int(m.group(1)) if m else -1\n",
    "    ckpt_files.sort(key=_epoch_from_path)\n",
    "    latest_ckpt = ckpt_files[-1]\n",
    "    ckpt = torch.load(latest_ckpt, map_location=device)\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    best_d1      = ckpt.get('best_d1', 0.0)\n",
    "    model_state  = ckpt['state_dict']\n",
    "    optim_state  = ckpt['optimizer']\n",
    "    scheduler_state = ckpt['scheduler']\n",
    "    print(f\"Resumed from {latest_ckpt} → starting epoch {start_epoch}, best_d1={best_d1:.3f}\")\n",
    "else:\n",
    "    ckpt = torch.load(pretrained_path, map_location=device)\n",
    "    start_epoch = 0\n",
    "    best_d1     = ckpt.get('best_d1', 0.0)\n",
    "    model_state = ckpt.get('state_dict', ckpt)\n",
    "    optim_state = None\n",
    "    scheduler_state = None\n",
    "    print(f\"Loaded pretrained weights from {pretrained_path}\")\n",
    "\n",
    "# Logger and TensorBoard\n",
    "logger = init_log(\"global\", logging.INFO)\n",
    "logger.propagate = 0\n",
    "writer = SummaryWriter(save_path)\n",
    "\n",
    "# Data loaders\n",
    "train_ds = CustomDepthDataset(train_list, root_dir, mode=\"train\",\n",
    "                              size=(cfg[\"img_size\"], cfg[\"img_size\"]),\n",
    "                              near=cfg[\"min_depth\"], far=cfg[\"max_depth\"])\n",
    "val_ds   = CustomDepthDataset(val_list,   root_dir, mode=\"val\",\n",
    "                              size=(cfg[\"img_size\"], cfg[\"img_size\"]),\n",
    "                              near=cfg[\"min_depth\"], far=cfg[\"max_depth\"])\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg[\"bs\"], shuffle=True,\n",
    "                          num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=1, shuffle=False,\n",
    "                          num_workers=2, pin_memory=True, drop_last=True)\n",
    "\n",
    "# Model\n",
    "model_cfgs = {\n",
    "    \"vits\": {\"encoder\":\"vits\",\"features\":64,  \"out_channels\":[48,96,192,384]},\n",
    "    \"vitb\": {\"encoder\":\"vitb\",\"features\":128, \"out_channels\":[96,192,384,768]},\n",
    "    \"vitl\": {\"encoder\":\"vitl\",\"features\":256, \"out_channels\":[256,512,1024,1024]},\n",
    "    \"vitg\": {\"encoder\":\"vitg\",\"features\":384, \"out_channels\":[1536]*4}\n",
    "}\n",
    "model = DepthAnythingV2(**{**model_cfgs[cfg[\"encoder\"]], \"max_depth\":cfg[\"max_depth\"]}).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "\n",
    "criterion = SiLogLoss().to(device)\n",
    "\n",
    "# Optimizer and scheduler setup\n",
    "optimizer = AdamW([\n",
    "    {\"params\":[p for n,p in model.named_parameters() if \"pretrained\" in n],     \"lr\":cfg[\"lr\"]},\n",
    "    {\"params\":[p for n,p in model.named_parameters() if \"pretrained\" not in n], \"lr\":cfg[\"lr\"]*10}\n",
    "], lr=cfg[\"lr\"], betas=(0.9,0.999), weight_decay=cfg.get(\"weight_decay\",0.01))\n",
    "if optim_state is not None:\n",
    "    optimizer.load_state_dict(optim_state)\n",
    "\n",
    "# Warmup and cosine\n",
    "total_steps = cfg[\"epochs\"] * len(train_loader)\n",
    "warmup_steps = cfg.get(\"lr_warmup\", 0) * len(train_loader)\n",
    "base_lrs = [g[\"lr\"] for g in optimizer.param_groups]\n",
    "scheduler = None\n",
    "if cfg.get(\"lr_scheduler\") == \"cosine\":\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps)\n",
    "    if scheduler_state is not None:\n",
    "        scheduler.load_state_dict(scheduler_state)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, cfg[\"epochs\"]):\n",
    "    if cfg.get(\"freeze_encoder\") and epoch < cfg.get(\"freeze_epochs\", 0):\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"pretrained\" in name:\n",
    "                param.requires_grad = False\n",
    "    elif cfg.get(\"freeze_encoder\") and epoch == cfg.get(\"freeze_epochs\", 0):\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"pretrained\" in name:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Rebuild train_loader with the smaller batch size\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=cfg[\"bs_unfrozen\"],\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        print(f\"Unfroze encoder — switching train batch_size to {cfg['bs_unfrozen']}\")\n",
    "\n",
    "        total_steps  = cfg[\"epochs\"] * len(train_loader)\n",
    "        warmup_steps = cfg.get(\"lr_warmup\", 0) * len(train_loader)\n",
    "\n",
    "    # Train pass\n",
    "    model.train()\n",
    "    train_bar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"[{epoch+1}/{cfg['epochs']}] train\",\n",
    "        ncols=80, miniters=100, unit=\"it\"\n",
    "    )\n",
    "    batch_losses = []\n",
    "    for i, sample in enumerate(train_bar):\n",
    "        img, depth, vm = (\n",
    "            sample[\"image\"].to(device),\n",
    "            sample[\"depth\"].to(device),\n",
    "            sample[\"valid_mask\"].to(device)\n",
    "        )\n",
    "        if random.random() < 0.5:\n",
    "            img, depth, vm = img.flip(-1), depth.flip(-1), vm.flip(-1)\n",
    "\n",
    "        pred = model(img)\n",
    "        pred = torch.clamp(pred, cfg[\"min_depth\"], cfg[\"max_depth\"])\n",
    "        mask = (vm == 1) & (depth >= cfg[\"min_depth\"]) & (depth <= cfg[\"max_depth\"])\n",
    "        loss = criterion(pred, depth, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        # LR warmup and scheduler step\n",
    "        step = epoch * len(train_loader) + i\n",
    "        if step < warmup_steps:\n",
    "            for j, g in enumerate(optimizer.param_groups):\n",
    "                g[\"lr\"] = base_lrs[j] * float(step) / float(warmup_steps)\n",
    "        elif scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        lr_now = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar(\"train/loss\", loss.item(), step)\n",
    "        if i % 100 == 0:\n",
    "            train_bar.set_postfix(loss=f\"{loss.item():.3f}\", lr=f\"{lr_now:.2e}\")\n",
    "\n",
    "    loss_history[f\"epoch_{epoch+1}\"] = batch_losses\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    names = [\"d1\",\"d2\",\"d3\",\"abs_rel\",\"sq_rel\",\"rmse\",\"rmse_log\",\"log10\",\"silog\"]\n",
    "    metrics = torch.zeros(len(names), device=device)\n",
    "    val_loss_total = 0.0\n",
    "    count = 0\n",
    "    val_bar = tqdm(\n",
    "        val_loader,\n",
    "        desc=f\"[{epoch+1}/{cfg['epochs']}] val\",\n",
    "        ncols=80, miniters=20, unit=\"it\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for sample in val_bar:\n",
    "            img   = sample[\"image\"].to(device).float()\n",
    "            depth = sample[\"depth\"][0].to(device)\n",
    "            vm    = sample[\"valid_mask\"][0].to(device)\n",
    "\n",
    "            p = model(img)\n",
    "            p = torch.clamp(p, cfg[\"min_depth\"], cfg[\"max_depth\"])\n",
    "            p = F.interpolate(p[:,None], depth.shape[-2:], mode=\"bilinear\", align_corners=True)[0,0]\n",
    "\n",
    "            m = (vm == 1) & (depth >= cfg[\"min_depth\"]) & (depth <= cfg[\"max_depth\"])\n",
    "            if m.sum() < 10:\n",
    "                val_bar.update()\n",
    "                continue\n",
    "\n",
    "            res = eval_depth(p[m], depth[m])\n",
    "            loss_eval = criterion(p[None], depth[None], m[None])\n",
    "            val_loss_total += loss_eval.item()\n",
    "            metrics += torch.tensor([res[n] for n in names], device=device)\n",
    "            count += 1\n",
    "            val_bar.set_postfix(d1=f\"{res['d1']:.3f}\")\n",
    "\n",
    "    avg = metrics / count\n",
    "    val_loss_avg = val_loss_total / count\n",
    "    logger.info(\n",
    "        \"Validation ▶ \" +\n",
    "        \", \".join(f\"{n}:{avg[i].item():.3f}\" for i,n in enumerate(names)) +\n",
    "        f\" | loss={val_loss_avg:.3f}\"\n",
    "    )\n",
    "    for i, n in enumerate(names):\n",
    "        writer.add_scalar(f\"val/{n}\", avg[i].item(), epoch)\n",
    "\n",
    "    # Checkpointing\n",
    "    key = f\"{cfg['encoder']}_epoch_{epoch+1}\"\n",
    "    metrics_history[key] = {name: avg[i].item() for i,name in enumerate(names)}\n",
    "    metrics_history[key][\"avg_loss\"] = val_loss_avg\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == cfg[\"epochs\"]:\n",
    "        ckpt_name = f\"checkpoint_{cfg['encoder']}_epoch_{epoch+1:03d}.pth\"\n",
    "        torch.save(\n",
    "            {'epoch': epoch, 'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(), 'best_d1': best_d1,\n",
    "             'scheduler': scheduler.state_dict()},\n",
    "            os.path.join(ckpt_path, ckpt_name)\n",
    "        )\n",
    "\n",
    "    if avg[0] > best_d1:\n",
    "        for fpath in glob.glob(os.path.join(ckpt_path, f\"best_{cfg['encoder']}_epoch_*.pth\")):\n",
    "            os.remove(fpath)\n",
    "        best_d1 = avg[0]\n",
    "        best_ckpt = f\"best_{cfg['encoder']}_epoch_{epoch+1:03d}.pth\"\n",
    "        torch.save(\n",
    "            {'epoch': epoch, 'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(), 'best_d1': best_d1,\n",
    "             'scheduler': scheduler.state_dict()},\n",
    "            os.path.join(ckpt_path, best_ckpt)\n",
    "        )\n",
    "\n",
    "    with open(metrics_json, 'w') as f:\n",
    "        json.dump(metrics_history, f, indent=2)\n",
    "    with open(loss_json_path, 'w') as f:\n",
    "        json.dump(loss_history, f, indent=2)\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1749829996774,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "pxEEllU9iXL8"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 958251,
     "status": "ok",
     "timestamp": 1749832723471,
     "user": {
      "displayName": "Denis Pop",
      "userId": "08151418788944575013"
     },
     "user_tz": -180
    },
    "id": "kQJeOJ6Dia_0",
    "outputId": "5385fb54-8092-4314-e729-586c57b8bada"
   },
   "outputs": [],
   "source": [
    "!python /content/Depth-Anything-V2/metric_depth/colab_train.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOPa2nj3OQ942p4gSOVXAWh",
   "collapsed_sections": [
    "DcXVV84Vsg74"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "mount_file_id": "1QzAmRKX1GYBDFaXbvzcqsYadReyeZVv-",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
